# AI Interview Platform --- Phase 1: Resume Ingestion (RAG Foundation)

## ğŸ¯ Objective of Phase 1

Build a production-grade resume ingestion pipeline that:

- Accepts resume uploads (PDF)
- Extracts and cleans text
- Chunks intelligently (not naive splitting)
- Generates embeddings
- Stores vectors in Qdrant
- Extracts structured skills using LLM
- Stores metadata in PostgreSQL

This phase builds the foundation for the RAG-powered interview engine.

---

# ğŸ— Architecture Overview

Client â†’ FastAPI â†’ Resume Service â†’\
PDF Parser â†’ Chunking Service â†’ Embedding Service â†’ Qdrant\
Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â â†˜\
Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Skill Extraction (LLM) â†’
Postgres

---

# ğŸ“ Project Structure

ai-interview-platform/

server/\
â”œâ”€â”€ app.py\
â”œâ”€â”€ config.py\
â”œâ”€â”€ database.py\
â”œâ”€â”€ models/\
â”œâ”€â”€ schemas/\
â”œâ”€â”€ routes/\
â”œâ”€â”€ services/\
â””â”€â”€ utils/

frontend/

---

# ğŸ”µ Step-by-Step Flow

## 1ï¸âƒ£ Upload Resume

Endpoint: POST /resume/upload

- Validate file type (PDF only)
- Limit file size (e.g., 5MB)
- Generate file hash (avoid duplicates)
- Store temporarily

---

## 2ï¸âƒ£ Extract Text

Use PyPDF to extract raw text.

Processing: - Remove extra whitespace - Normalize newlines - Remove
broken characters - Preserve section structure if possible

Why this matters: Bad extraction = bad embeddings = bad retrieval.

---

## 3ï¸âƒ£ Intelligent Chunking

Avoid fixed 500-token splitting.

Recommended: - \~800 token chunks - 100--150 token overlap - Preserve
section continuity

Why overlap? Ensures semantic continuity between chunks.

Output: List of chunked text segments.

---

## 4ï¸âƒ£ Generate Embeddings

Use one embedding model consistently.

---

## 5ï¸âƒ£ Store in Qdrant

Collection: resume_embeddings

Each vector entry:

{ id: UUID, vector: embedding, payload: { user_id, resume_id,
chunk_text, chunk_index } }

Why payload? Allows filtered search later (per user, per resume).

---

## 7ï¸âƒ£ Store Resume Metadata

Table Fields:

- id
- user_id
- file_name
- resume_hash
- created_at

---

# ğŸ§  Why Phase 1 Is Critical

Most RAG systems fail because ingestion is weak.

This phase ensures:

- Clean semantic chunks
- Proper vector storage
- Structured metadata
- Retrieval-ready architecture
- Production-ready pipeline

---

# ğŸš€ Phase 1 Completion Checklist

âœ… Resume upload working\
âœ… Text extraction validated\
âœ… Chunks stored in memory correctly\
âœ… Embeddings generated\
âœ… Vectors stored in Qdrant\
âœ… Duplicate resume detection working
